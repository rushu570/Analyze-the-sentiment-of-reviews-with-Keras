{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.datasets import imdb\ntop_words = 10000\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_train[0]",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "[1,\n 14,\n 22,\n 16,\n 43,\n 530,\n 973,\n 1622,\n 1385,\n 65,\n 458,\n 4468,\n 66,\n 3941,\n 4,\n 173,\n 36,\n 256,\n 5,\n 25,\n 100,\n 43,\n 838,\n 112,\n 50,\n 670,\n 2,\n 9,\n 35,\n 480,\n 284,\n 5,\n 150,\n 4,\n 172,\n 112,\n 167,\n 2,\n 336,\n 385,\n 39,\n 4,\n 172,\n 4536,\n 1111,\n 17,\n 546,\n 38,\n 13,\n 447,\n 4,\n 192,\n 50,\n 16,\n 6,\n 147,\n 2025,\n 19,\n 14,\n 22,\n 4,\n 1920,\n 4613,\n 469,\n 4,\n 22,\n 71,\n 87,\n 12,\n 16,\n 43,\n 530,\n 38,\n 76,\n 15,\n 13,\n 1247,\n 4,\n 22,\n 17,\n 515,\n 17,\n 12,\n 16,\n 626,\n 18,\n 2,\n 5,\n 62,\n 386,\n 12,\n 8,\n 316,\n 8,\n 106,\n 5,\n 4,\n 2223,\n 5244,\n 16,\n 480,\n 66,\n 3785,\n 33,\n 4,\n 130,\n 12,\n 16,\n 38,\n 619,\n 5,\n 25,\n 124,\n 51,\n 36,\n 135,\n 48,\n 25,\n 1415,\n 33,\n 6,\n 22,\n 12,\n 215,\n 28,\n 77,\n 52,\n 5,\n 14,\n 407,\n 16,\n 82,\n 2,\n 8,\n 4,\n 107,\n 117,\n 5952,\n 15,\n 256,\n 4,\n 2,\n 7,\n 3766,\n 5,\n 723,\n 36,\n 71,\n 43,\n 530,\n 476,\n 26,\n 400,\n 317,\n 46,\n 7,\n 4,\n 2,\n 1029,\n 13,\n 104,\n 88,\n 4,\n 381,\n 15,\n 297,\n 98,\n 32,\n 2071,\n 56,\n 26,\n 141,\n 6,\n 194,\n 7486,\n 18,\n 4,\n 226,\n 22,\n 21,\n 134,\n 476,\n 26,\n 480,\n 5,\n 144,\n 30,\n 5535,\n 18,\n 51,\n 36,\n 28,\n 224,\n 92,\n 25,\n 104,\n 4,\n 226,\n 65,\n 16,\n 38,\n 1334,\n 88,\n 12,\n 16,\n 283,\n 5,\n 16,\n 4472,\n 113,\n 103,\n 32,\n 15,\n 16,\n 5345,\n 19,\n 178,\n 32]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing import sequence\nmax_review_length = 500\nx_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\nx_test = sequence.pad_sequences(x_test, maxlen=max_review_length)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Flatten\n\nembedding_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nprint(model.summary())",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 500, 32)           320000    \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 16000)             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                256016    \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                272       \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 576,305\nTrainable params: 576,305\nNon-trainable params: 0\n_________________________________________________________________\nNone\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hist = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=5, batch_size=128)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 25000 samples, validate on 25000 samples\nEpoch 1/5\n25000/25000 [==============================] - 12s 474us/step - loss: 0.5869 - acc: 0.6443 - val_loss: 0.3401 - val_acc: 0.8494\nEpoch 2/5\n25000/25000 [==============================] - 12s 463us/step - loss: 0.2266 - acc: 0.9108 - val_loss: 0.2757 - val_acc: 0.8843\nEpoch 3/5\n25000/25000 [==============================] - 12s 464us/step - loss: 0.0949 - acc: 0.9720 - val_loss: 0.3245 - val_acc: 0.8783\nEpoch 4/5\n25000/25000 [==============================] - 11s 434us/step - loss: 0.0254 - acc: 0.9952 - val_loss: 0.4254 - val_acc: 0.8679\nEpoch 5/5\n25000/25000 [==============================] - 12s 489us/step - loss: 0.0067 - acc: 0.9990 - val_loss: 0.4797 - val_acc: 0.8693\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scores = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1] * 100))",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Accuracy: 86.93%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import string\nimport numpy as np\n\ndef analyze(text):\n    # Prepare the input by removing punctuation characters, converting\n    # characters to lower case, and removing words containing numbers\n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n    text = text.lower().split(' ')\n    text = [word for word in text if word.isalpha()]\n\n    # Generate an input tensor\n    input = [1]\n    for word in text:\n        if word in word_dict and word_dict[word] < top_words:\n            input.append(word_dict[word])\n        else:\n            input.append(2)\n    padded_input = sequence.pad_sequences([input], maxlen=max_review_length)\n\n    # Invoke the model and return the result\n    result = model.predict(np.array([padded_input][0]))[0][0]\n    return result",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "analyze('Easily the most stellar experience I have ever had.')",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'word_dict' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-5bc88cfb4aee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Easily the most stellar experience I have ever had.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-79e3c210b005>\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_dict' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}